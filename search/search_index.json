{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"En construction... Contenu \uf0c1 Articles en g\u00e9n\u00e9ral tr\u00e8s courts, qui donnent un point de vue sur un concept. Sachant le nom d'un concept, il est alors tr\u00e8s facile aujourd'hui pour le lecteur de rechercher et de trouver rapidement tous les d\u00e9tails de ce concept. Il y a donc peu de liens externes dans ce dictionnaire. Aucun nom de personnalit\u00e9, entreprise ou universit\u00e9 n'est donn\u00e9 dans ce dictionnaire, qui est focalis\u00e9 enti\u00e8rement sur des concepts scientifiques. Plusieurs raisons motivent ce choix, qui reviennent souvent \u00e0 un id\u00e9al personnel dans lequel seules les id\u00e9es sont cit\u00e9es et publi\u00e9es anonymement, avec humilit\u00e9 et \u00e0 l'exclusion de tout culte de la personnalit\u00e9. Licence \uf0c1 CC-BY Comment citer \uf0c1 Depuis une page web: https://cerisara.github.io/dldico Depuis un code source: https://github.com/cerisara/dldico Depuis un article: (une version preprint du dictionnaire sera bient\u00f4t publi\u00e9e dans HAL)","title":"Home"},{"location":"#contenu","text":"Articles en g\u00e9n\u00e9ral tr\u00e8s courts, qui donnent un point de vue sur un concept. Sachant le nom d'un concept, il est alors tr\u00e8s facile aujourd'hui pour le lecteur de rechercher et de trouver rapidement tous les d\u00e9tails de ce concept. Il y a donc peu de liens externes dans ce dictionnaire. Aucun nom de personnalit\u00e9, entreprise ou universit\u00e9 n'est donn\u00e9 dans ce dictionnaire, qui est focalis\u00e9 enti\u00e8rement sur des concepts scientifiques. Plusieurs raisons motivent ce choix, qui reviennent souvent \u00e0 un id\u00e9al personnel dans lequel seules les id\u00e9es sont cit\u00e9es et publi\u00e9es anonymement, avec humilit\u00e9 et \u00e0 l'exclusion de tout culte de la personnalit\u00e9.","title":"Contenu"},{"location":"#licence","text":"CC-BY","title":"Licence"},{"location":"#comment-citer","text":"Depuis une page web: https://cerisara.github.io/dldico Depuis un code source: https://github.com/cerisara/dldico Depuis un article: (une version preprint du dictionnaire sera bient\u00f4t publi\u00e9e dans HAL)","title":"Comment citer"},{"location":"d/","text":"Deep learning \uf0c1 R\u00e9duire le deep learning \u00e0 la seule application des r\u00e9seaux de neurones invent\u00e9s dans les ann\u00e9es 80 \u00e0 de grandes bases de donn\u00e9es sur des GPU puissants, est \u00e9quivalent \u00e0 dire que les langages de programmation actuels (python, go, caml...) ne sont que l'application d'instructions en langage machine dans de grandes m\u00e9moires RAM, ou que \"A la recherche du temps perdu\" n'est qu'une suite de lettres sur de nombreuses pages. Voir aussi Programmation diff\u00e9rentiable Differential privacy \uf0c1 Le principe de base est d'ajouter du bruit pendant l'apprentissage des mod\u00e8les, ce qui permet de r\u00e9duire th\u00e9oriquement la probabilit\u00e9 que le mod\u00e8le divulgue des donn\u00e9es apprises par coeur, pr\u00e9venant ainsi des attaques sur les mod\u00e8les de deep learning comme la \"membership inference attack\". Mais le bruit r\u00e9duit aussi la pr\u00e9cision du mod\u00e8le pour la t\u00e2che qu'il est cens\u00e9 r\u00e9aliser, et cette r\u00e9duction des performances se r\u00e9v\u00e8le souvent trop importante pour \u00eatre utilisable en pratique. Distillation \uf0c1 Les gros mod\u00e8les (teacher) peuvent \u00eatre \"r\u00e9duits\" en transmettant les informations qu'ils ont apprises \u00e0 d'autres mod\u00e8les plus petits (students) mais dont les performances sont quasi-identiques. D\u00e9but 2021, on parle aussi de distillation de donn\u00e9es, c'est-\u00e0-dire de la possibilit\u00e9 de construire de toutes petites bases de donn\u00e9es artificielles qui reproduisent les gradients des gros mod\u00e8les. Cette approche permet d'entra\u00eener ensuite un mod\u00e8le sur tr\u00e8s peu de donn\u00e9es et tr\u00e8s rapidement. A mon avis, cette piste est int\u00e9ressante, mais d'autres papiers d\u00e9montrent que pour bien g\u00e9n\u00e9raliser, les mod\u00e8les doivent apprendre par coeur la longue queue de la loi de Zipf, et je ne suis pas certain que la distillation de datasets le permette (?)","title":"D"},{"location":"d/#deep-learning","text":"R\u00e9duire le deep learning \u00e0 la seule application des r\u00e9seaux de neurones invent\u00e9s dans les ann\u00e9es 80 \u00e0 de grandes bases de donn\u00e9es sur des GPU puissants, est \u00e9quivalent \u00e0 dire que les langages de programmation actuels (python, go, caml...) ne sont que l'application d'instructions en langage machine dans de grandes m\u00e9moires RAM, ou que \"A la recherche du temps perdu\" n'est qu'une suite de lettres sur de nombreuses pages. Voir aussi Programmation diff\u00e9rentiable","title":"Deep learning"},{"location":"d/#differential-privacy","text":"Le principe de base est d'ajouter du bruit pendant l'apprentissage des mod\u00e8les, ce qui permet de r\u00e9duire th\u00e9oriquement la probabilit\u00e9 que le mod\u00e8le divulgue des donn\u00e9es apprises par coeur, pr\u00e9venant ainsi des attaques sur les mod\u00e8les de deep learning comme la \"membership inference attack\". Mais le bruit r\u00e9duit aussi la pr\u00e9cision du mod\u00e8le pour la t\u00e2che qu'il est cens\u00e9 r\u00e9aliser, et cette r\u00e9duction des performances se r\u00e9v\u00e8le souvent trop importante pour \u00eatre utilisable en pratique.","title":"Differential privacy"},{"location":"d/#distillation","text":"Les gros mod\u00e8les (teacher) peuvent \u00eatre \"r\u00e9duits\" en transmettant les informations qu'ils ont apprises \u00e0 d'autres mod\u00e8les plus petits (students) mais dont les performances sont quasi-identiques. D\u00e9but 2021, on parle aussi de distillation de donn\u00e9es, c'est-\u00e0-dire de la possibilit\u00e9 de construire de toutes petites bases de donn\u00e9es artificielles qui reproduisent les gradients des gros mod\u00e8les. Cette approche permet d'entra\u00eener ensuite un mod\u00e8le sur tr\u00e8s peu de donn\u00e9es et tr\u00e8s rapidement. A mon avis, cette piste est int\u00e9ressante, mais d'autres papiers d\u00e9montrent que pour bien g\u00e9n\u00e9raliser, les mod\u00e8les doivent apprendre par coeur la longue queue de la loi de Zipf, et je ne suis pas certain que la distillation de datasets le permette (?)","title":"Distillation"},{"location":"f/","text":"Federated learning \uf0c1 Comment continuer \u00e0 entra\u00eener des mod\u00e8les sans divulguer les donn\u00e9es priv\u00e9es ? Comment se passer des fermes de GPU au co\u00fbt \u00e9cologique important ? Une solution \u00e0 ces deux probl\u00e8mes (et \u00e0 d'autres) pourrait bien \u00eatre le federated learning (FL), qui apprend les mod\u00e8les localement et les fusionne ensuite soit globalement, soit de mani\u00e8re d\u00e9centralis\u00e9e. D'innombrables variantes de ce principe existent, les plus connues \u00e9tant connues sous le nom de \"collaborative learning\" et \"fusion learning\". Il existe de nombreuses formes de FL: cross-silo lorsque les noeuds de calcul locaux sont puissants, cross-device lorsque l'essentiel du calcul est d\u00e9port\u00e9, horizontal lorsque les donn\u00e9es sont distribu\u00e9es ind\u00e9pendamment entre les noeuds, vertical lorsque les noeuds collaborent sur des donn\u00e9es en partie partag\u00e9es, etc.","title":"F"},{"location":"f/#federated-learning","text":"Comment continuer \u00e0 entra\u00eener des mod\u00e8les sans divulguer les donn\u00e9es priv\u00e9es ? Comment se passer des fermes de GPU au co\u00fbt \u00e9cologique important ? Une solution \u00e0 ces deux probl\u00e8mes (et \u00e0 d'autres) pourrait bien \u00eatre le federated learning (FL), qui apprend les mod\u00e8les localement et les fusionne ensuite soit globalement, soit de mani\u00e8re d\u00e9centralis\u00e9e. D'innombrables variantes de ce principe existent, les plus connues \u00e9tant connues sous le nom de \"collaborative learning\" et \"fusion learning\". Il existe de nombreuses formes de FL: cross-silo lorsque les noeuds de calcul locaux sont puissants, cross-device lorsque l'essentiel du calcul est d\u00e9port\u00e9, horizontal lorsque les donn\u00e9es sont distribu\u00e9es ind\u00e9pendamment entre les noeuds, vertical lorsque les noeuds collaborent sur des donn\u00e9es en partie partag\u00e9es, etc.","title":"Federated learning"},{"location":"g/","text":"GAN \uf0c1 Generative Adversarial Network: plut\u00f4t que g\u00e9n\u00e9rer des donn\u00e9es en minimisant simplement la distance entre les donn\u00e9es g\u00e9n\u00e9r\u00e9es et les exemples r\u00e9els (c'est l'apprentissage classique), un bien meilleur g\u00e9n\u00e9rateur peut \u00eatre obtenu en lui apprenant \u00e0 g\u00e9n\u00e9rer des exemples qui trompent un autre r\u00e9seau, qui est lui entra\u00een\u00e9 \u00e0 discriminer les exemples r\u00e9els des faux. Pas facile \u00e0 faire converger, mais d'innombrables progr\u00e8s ont \u00e9t\u00e9 r\u00e9alis\u00e9s (cf. par ex. Wasserstein-GAN).","title":"G"},{"location":"g/#gan","text":"Generative Adversarial Network: plut\u00f4t que g\u00e9n\u00e9rer des donn\u00e9es en minimisant simplement la distance entre les donn\u00e9es g\u00e9n\u00e9r\u00e9es et les exemples r\u00e9els (c'est l'apprentissage classique), un bien meilleur g\u00e9n\u00e9rateur peut \u00eatre obtenu en lui apprenant \u00e0 g\u00e9n\u00e9rer des exemples qui trompent un autre r\u00e9seau, qui est lui entra\u00een\u00e9 \u00e0 discriminer les exemples r\u00e9els des faux. Pas facile \u00e0 faire converger, mais d'innombrables progr\u00e8s ont \u00e9t\u00e9 r\u00e9alis\u00e9s (cf. par ex. Wasserstein-GAN).","title":"GAN"},{"location":"p/","text":"Programmation diff\u00e9rentiable \uf0c1 Le deep learning peut \u00eatre vu comme une extension de la programmation informatique. Un mod\u00e8le de deep learning est un programme contenant des param\u00e8tres, qui sont optimis\u00e9s automatiquement sur des donn\u00e9es. Cette optimisation est r\u00e9alis\u00e9e gr\u00e2ce \u00e0 des techniques de diff\u00e9rentiation automatique de la fonction math\u00e9matique cod\u00e9e par le programme. D'o\u00f9 le nom de \"programmation diff\u00e9rentiable\" qui a parfois \u00e9t\u00e9 donn\u00e9 au deep learning.","title":"P"},{"location":"p/#programmation-differentiable","text":"Le deep learning peut \u00eatre vu comme une extension de la programmation informatique. Un mod\u00e8le de deep learning est un programme contenant des param\u00e8tres, qui sont optimis\u00e9s automatiquement sur des donn\u00e9es. Cette optimisation est r\u00e9alis\u00e9e gr\u00e2ce \u00e0 des techniques de diff\u00e9rentiation automatique de la fonction math\u00e9matique cod\u00e9e par le programme. D'o\u00f9 le nom de \"programmation diff\u00e9rentiable\" qui a parfois \u00e9t\u00e9 donn\u00e9 au deep learning.","title":"Programmation diff\u00e9rentiable"},{"location":"t/","text":"Transformer \uf0c1 Le mod\u00e8le qui a compl\u00e8tement remplac\u00e9 toute la famille des r\u00e9seaux r\u00e9currents (RNN, LSTM, GRU...) par une nouvelle famille bas\u00e9e sur la self-attention. Pourquoi les transformers sont meilleurs ? Parce que les empiler permet de capturer l'information provenant de corpus textuels de plus en plus gigantesques, tout comme les CNN avec les gros corpus d'images. Les transformers ont r\u00e9volutionn\u00e9 le TAL, et sont \u00e0 la base de tous les mod\u00e8les r\u00e9cents du domaine: BERT, GPT-1.2.3, RoBERTa, ALBERT, XL-NET, T5, BART...","title":"T"},{"location":"t/#transformer","text":"Le mod\u00e8le qui a compl\u00e8tement remplac\u00e9 toute la famille des r\u00e9seaux r\u00e9currents (RNN, LSTM, GRU...) par une nouvelle famille bas\u00e9e sur la self-attention. Pourquoi les transformers sont meilleurs ? Parce que les empiler permet de capturer l'information provenant de corpus textuels de plus en plus gigantesques, tout comme les CNN avec les gros corpus d'images. Les transformers ont r\u00e9volutionn\u00e9 le TAL, et sont \u00e0 la base de tous les mod\u00e8les r\u00e9cents du domaine: BERT, GPT-1.2.3, RoBERTa, ALBERT, XL-NET, T5, BART...","title":"Transformer"},{"location":"z/","text":"Zero-shot learning \uf0c1 En TAL, l'une des marottes du d\u00e9but 2021 consiste \u00e0 exploiter les gros transformers appris sur tout l'anglais (T5, BART...) pour r\u00e9soudre des t\u00e2ches de NLP sans apprentissage, simplement en ajoutant la description D de la t\u00e2che en anglais aux observations X , le mod\u00e8le \u00e9tant capable d'inf\u00e9rer le lien s\u00e9mantique entre D,X et la solution Y . Refs: adapters entailment","title":"Z"},{"location":"z/#zero-shot-learning","text":"En TAL, l'une des marottes du d\u00e9but 2021 consiste \u00e0 exploiter les gros transformers appris sur tout l'anglais (T5, BART...) pour r\u00e9soudre des t\u00e2ches de NLP sans apprentissage, simplement en ajoutant la description D de la t\u00e2che en anglais aux observations X , le mod\u00e8le \u00e9tant capable d'inf\u00e9rer le lien s\u00e9mantique entre D,X et la solution Y . Refs: adapters entailment","title":"Zero-shot learning"}]}