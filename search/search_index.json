{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"En construction... Contenu \uf0c1 Langue des articles: fran\u00e7ais \"augment\u00e9\" de termes anglais, car le domaine du deep learning contient de tr\u00e8s nombreux termes techniques anglais (\u00e0 commencer par son nom), qui perdent beaucoup de leur sens et de leur contexte d'utilisation lorsqu'ils sont traduits. Articles en g\u00e9n\u00e9ral tr\u00e8s courts, qui donnent un point de vue sur un concept. Sachant le nom d'un concept, il est alors tr\u00e8s facile aujourd'hui pour le lecteur de rechercher et de trouver rapidement tous les d\u00e9tails de ce concept. Il y a donc peu de liens externes dans ce dictionnaire. Aucun nom de personnalit\u00e9, entreprise ou universit\u00e9 n'est donn\u00e9 dans ce dictionnaire, qui est focalis\u00e9 enti\u00e8rement sur des concepts scientifiques. Plusieurs raisons motivent ce choix, qui reviennent souvent \u00e0 un id\u00e9al personnel dans lequel seules les id\u00e9es sont cit\u00e9es et publi\u00e9es anonymement, avec humilit\u00e9 et \u00e0 l'exclusion de tout culte de la personnalit\u00e9. Licence \uf0c1 CC-BY Comment citer \uf0c1 Depuis une page web: https://cerisara.github.io/dldico Depuis un code source: https://github.com/cerisara/dldico Depuis un article: (une version preprint du dictionnaire sera bient\u00f4t publi\u00e9e dans HAL) Comment contribuer \uf0c1 Si vous aimez, vous pouvez ajouter une \"\u00e9toile\" dans github Si vous voulez commenter, vous pouvez ajouter une \"issue\" dans github Si vous voulez ajouter ou modifier un article, vous pouvez faire un \"pull request\" dans github Toute contribution est bienvenue, \u00e0 partir du moment o\u00f9 elle s'inscrit dans la \"ligne \u00e9ditoriale\" r\u00e9sum\u00e9e ci-dessus. Toute contributrice ou tout contributeur est list\u00e9 en tant que tel par github.","title":"Home"},{"location":"#contenu","text":"Langue des articles: fran\u00e7ais \"augment\u00e9\" de termes anglais, car le domaine du deep learning contient de tr\u00e8s nombreux termes techniques anglais (\u00e0 commencer par son nom), qui perdent beaucoup de leur sens et de leur contexte d'utilisation lorsqu'ils sont traduits. Articles en g\u00e9n\u00e9ral tr\u00e8s courts, qui donnent un point de vue sur un concept. Sachant le nom d'un concept, il est alors tr\u00e8s facile aujourd'hui pour le lecteur de rechercher et de trouver rapidement tous les d\u00e9tails de ce concept. Il y a donc peu de liens externes dans ce dictionnaire. Aucun nom de personnalit\u00e9, entreprise ou universit\u00e9 n'est donn\u00e9 dans ce dictionnaire, qui est focalis\u00e9 enti\u00e8rement sur des concepts scientifiques. Plusieurs raisons motivent ce choix, qui reviennent souvent \u00e0 un id\u00e9al personnel dans lequel seules les id\u00e9es sont cit\u00e9es et publi\u00e9es anonymement, avec humilit\u00e9 et \u00e0 l'exclusion de tout culte de la personnalit\u00e9.","title":"Contenu"},{"location":"#licence","text":"CC-BY","title":"Licence"},{"location":"#comment-citer","text":"Depuis une page web: https://cerisara.github.io/dldico Depuis un code source: https://github.com/cerisara/dldico Depuis un article: (une version preprint du dictionnaire sera bient\u00f4t publi\u00e9e dans HAL)","title":"Comment citer"},{"location":"#comment-contribuer","text":"Si vous aimez, vous pouvez ajouter une \"\u00e9toile\" dans github Si vous voulez commenter, vous pouvez ajouter une \"issue\" dans github Si vous voulez ajouter ou modifier un article, vous pouvez faire un \"pull request\" dans github Toute contribution est bienvenue, \u00e0 partir du moment o\u00f9 elle s'inscrit dans la \"ligne \u00e9ditoriale\" r\u00e9sum\u00e9e ci-dessus. Toute contributrice ou tout contributeur est list\u00e9 en tant que tel par github.","title":"Comment contribuer"},{"location":"b/","text":"BART \uf0c1 Famille de mod\u00e8les seq2seq bas\u00e9s sur les transformers. Voir T5 .","title":"B"},{"location":"b/#bart","text":"Famille de mod\u00e8les seq2seq bas\u00e9s sur les transformers. Voir T5 .","title":"BART"},{"location":"d/","text":"Deep learning \uf0c1 R\u00e9duire le deep learning \u00e0 la seule application des r\u00e9seaux de neurones invent\u00e9s dans les ann\u00e9es 80 \u00e0 de grandes bases de donn\u00e9es sur des GPU puissants, est \u00e9quivalent \u00e0 dire que les langages de programmation actuels (python, go, caml...) ne sont que l'application d'instructions en langage machine dans de grandes m\u00e9moires RAM, ou que \"A la recherche du temps perdu\" n'est qu'une suite de lettres sur de nombreuses pages. Le deep learning est une \u00e9volution majeure du domaine du machine learning, bas\u00e9e sur trois notions fondamentales qui existaient depuis longtemps, mais dont la combinaison et surtout l'usage novateur a pos\u00e9 les bases du deep learning: (i) la d\u00e9rivation automatique, qui permet d'optimiser de tr\u00e8s nombreuses familles de fonctions sans avoir besoin de calculer leur gradient; (ii) l'apprentissage de bout-en-bout ( end-to-end ), qui permet de combiner toutes les fonctions et sous-modules que l'on souhaite sans avoir \u00e0 les optimiser les uns apr\u00e8s les autres; et (iii) l'algorithme de descente de gradient stochastique ( SGD ), qui s'est r\u00e9v\u00e9l\u00e9 d'une efficacit\u00e9 redoutable pour optimiser de telles architectures. Alors que la recherche en machine learning se focalisait sur l'extraction de features les plus pertinentes possibles pour quelques mod\u00e8les pr\u00e9d\u00e9finis \u00e9quip\u00e9s d'algorithmes d'optimisation sp\u00e9cifiques, le deep learning a int\u00e9gr\u00e9 le calcul de ces features au sein du mod\u00e8le et s'est focalis\u00e9 sur la conception du mod\u00e8le en entier, optimis\u00e9 grace au seul algorithme SGD. A ceci s'ajoute un contexte favorable: de grandes masses de donn\u00e9es, et des GPUs \u00e0 foison, qui rendent r\u00e9alisables l'apprentissage de mod\u00e8les de plus en plus gros qui acqui\u00e8rent des informations de plus en plus pr\u00e9cises. Ce changement d'\u00e9chelle a lev\u00e9 un voile sur un nouveau champ d'investigation th\u00e9orique en machine learning, inaccessible jusque l\u00e0: la convexit\u00e9, jusque l\u00e0 consid\u00e9r\u00e9e comme importante, ne l'est plus du tout. les optima locaux, jusque l\u00e0 \u00e0 proscrire, s'av\u00e8rent finalement des alli\u00e9s pr\u00e9cieux; les \"saddle points\" devenant les nouveaux ennemis; l'overfitting, jusque l\u00e0 \u00e0 \u00e9viter \u00e0 tout prix, devient un passage oblig\u00e9 vers le succ\u00e8s (voir la double descente ). Et surtout, comment expliquer les \u00e9tonnantes capacit\u00e9s de g\u00e9n\u00e9ralisation ainsi obtenues ? Quelles propri\u00e9t\u00e9s g\u00e9om\u00e9triques poss\u00e8de vraiment l'espace en tr\u00e8s grande dimension des param\u00e8tres ? Ce nouvel eldorado th\u00e9orique est explor\u00e9 actuellement par de nombreux chercheurs (cf. par ex. KK , NT , SM ...) mais reste bien myst\u00e9rieux. Voir aussi Programmation diff\u00e9rentiable Differential privacy \uf0c1 Le principe de base est d'ajouter du bruit pendant l'apprentissage des mod\u00e8les, ce qui permet de r\u00e9duire th\u00e9oriquement la probabilit\u00e9 que le mod\u00e8le divulgue des donn\u00e9es apprises par coeur, pr\u00e9venant ainsi des attaques sur les mod\u00e8les de deep learning comme la \"membership inference attack\". Mais le bruit r\u00e9duit aussi la pr\u00e9cision du mod\u00e8le pour la t\u00e2che qu'il est cens\u00e9 r\u00e9aliser, et cette r\u00e9duction des performances se r\u00e9v\u00e8le souvent trop importante pour \u00eatre utilisable en pratique. Distillation \uf0c1 Les gros mod\u00e8les (teacher) peuvent \u00eatre \"r\u00e9duits\" en transmettant les informations qu'ils ont apprises \u00e0 d'autres mod\u00e8les plus petits (students) mais dont les performances sont quasi-identiques. D\u00e9but 2021, on parle aussi de distillation de donn\u00e9es, c'est-\u00e0-dire de la possibilit\u00e9 de construire de toutes petites bases de donn\u00e9es artificielles qui reproduisent les gradients des gros mod\u00e8les. Cette approche permet d'entra\u00eener ensuite un mod\u00e8le sur tr\u00e8s peu de donn\u00e9es et tr\u00e8s rapidement. A mon avis, cette piste est int\u00e9ressante, mais d'autres papiers d\u00e9montrent que pour bien g\u00e9n\u00e9raliser, les mod\u00e8les doivent apprendre par coeur la longue queue de la loi de Zipf, et je ne suis pas certain que la distillation de datasets le permette (?)","title":"D"},{"location":"d/#deep-learning","text":"R\u00e9duire le deep learning \u00e0 la seule application des r\u00e9seaux de neurones invent\u00e9s dans les ann\u00e9es 80 \u00e0 de grandes bases de donn\u00e9es sur des GPU puissants, est \u00e9quivalent \u00e0 dire que les langages de programmation actuels (python, go, caml...) ne sont que l'application d'instructions en langage machine dans de grandes m\u00e9moires RAM, ou que \"A la recherche du temps perdu\" n'est qu'une suite de lettres sur de nombreuses pages. Le deep learning est une \u00e9volution majeure du domaine du machine learning, bas\u00e9e sur trois notions fondamentales qui existaient depuis longtemps, mais dont la combinaison et surtout l'usage novateur a pos\u00e9 les bases du deep learning: (i) la d\u00e9rivation automatique, qui permet d'optimiser de tr\u00e8s nombreuses familles de fonctions sans avoir besoin de calculer leur gradient; (ii) l'apprentissage de bout-en-bout ( end-to-end ), qui permet de combiner toutes les fonctions et sous-modules que l'on souhaite sans avoir \u00e0 les optimiser les uns apr\u00e8s les autres; et (iii) l'algorithme de descente de gradient stochastique ( SGD ), qui s'est r\u00e9v\u00e9l\u00e9 d'une efficacit\u00e9 redoutable pour optimiser de telles architectures. Alors que la recherche en machine learning se focalisait sur l'extraction de features les plus pertinentes possibles pour quelques mod\u00e8les pr\u00e9d\u00e9finis \u00e9quip\u00e9s d'algorithmes d'optimisation sp\u00e9cifiques, le deep learning a int\u00e9gr\u00e9 le calcul de ces features au sein du mod\u00e8le et s'est focalis\u00e9 sur la conception du mod\u00e8le en entier, optimis\u00e9 grace au seul algorithme SGD. A ceci s'ajoute un contexte favorable: de grandes masses de donn\u00e9es, et des GPUs \u00e0 foison, qui rendent r\u00e9alisables l'apprentissage de mod\u00e8les de plus en plus gros qui acqui\u00e8rent des informations de plus en plus pr\u00e9cises. Ce changement d'\u00e9chelle a lev\u00e9 un voile sur un nouveau champ d'investigation th\u00e9orique en machine learning, inaccessible jusque l\u00e0: la convexit\u00e9, jusque l\u00e0 consid\u00e9r\u00e9e comme importante, ne l'est plus du tout. les optima locaux, jusque l\u00e0 \u00e0 proscrire, s'av\u00e8rent finalement des alli\u00e9s pr\u00e9cieux; les \"saddle points\" devenant les nouveaux ennemis; l'overfitting, jusque l\u00e0 \u00e0 \u00e9viter \u00e0 tout prix, devient un passage oblig\u00e9 vers le succ\u00e8s (voir la double descente ). Et surtout, comment expliquer les \u00e9tonnantes capacit\u00e9s de g\u00e9n\u00e9ralisation ainsi obtenues ? Quelles propri\u00e9t\u00e9s g\u00e9om\u00e9triques poss\u00e8de vraiment l'espace en tr\u00e8s grande dimension des param\u00e8tres ? Ce nouvel eldorado th\u00e9orique est explor\u00e9 actuellement par de nombreux chercheurs (cf. par ex. KK , NT , SM ...) mais reste bien myst\u00e9rieux. Voir aussi Programmation diff\u00e9rentiable","title":"Deep learning"},{"location":"d/#differential-privacy","text":"Le principe de base est d'ajouter du bruit pendant l'apprentissage des mod\u00e8les, ce qui permet de r\u00e9duire th\u00e9oriquement la probabilit\u00e9 que le mod\u00e8le divulgue des donn\u00e9es apprises par coeur, pr\u00e9venant ainsi des attaques sur les mod\u00e8les de deep learning comme la \"membership inference attack\". Mais le bruit r\u00e9duit aussi la pr\u00e9cision du mod\u00e8le pour la t\u00e2che qu'il est cens\u00e9 r\u00e9aliser, et cette r\u00e9duction des performances se r\u00e9v\u00e8le souvent trop importante pour \u00eatre utilisable en pratique.","title":"Differential privacy"},{"location":"d/#distillation","text":"Les gros mod\u00e8les (teacher) peuvent \u00eatre \"r\u00e9duits\" en transmettant les informations qu'ils ont apprises \u00e0 d'autres mod\u00e8les plus petits (students) mais dont les performances sont quasi-identiques. D\u00e9but 2021, on parle aussi de distillation de donn\u00e9es, c'est-\u00e0-dire de la possibilit\u00e9 de construire de toutes petites bases de donn\u00e9es artificielles qui reproduisent les gradients des gros mod\u00e8les. Cette approche permet d'entra\u00eener ensuite un mod\u00e8le sur tr\u00e8s peu de donn\u00e9es et tr\u00e8s rapidement. A mon avis, cette piste est int\u00e9ressante, mais d'autres papiers d\u00e9montrent que pour bien g\u00e9n\u00e9raliser, les mod\u00e8les doivent apprendre par coeur la longue queue de la loi de Zipf, et je ne suis pas certain que la distillation de datasets le permette (?)","title":"Distillation"},{"location":"e/","text":"Embeddings \uf0c1 ( Fr: plongements ) Ce terme fait r\u00e9f\u00e9rence \u00e0 une repr\u00e9sentation vectorielle des observations. Il est souvent (mais pas seulement) utilis\u00e9 en TAL pour repr\u00e9senter les mots, ou les phrases, ou les documents, avec un seul vecteur. Avant 2018, un mot du lexique \u00e9tait souvent repr\u00e9sent\u00e9 par un unique vecteur, toujours le m\u00eame, par exemple calcul\u00e9 avec les mod\u00e8les Word2Vec ou Glove. D'autres mod\u00e8les existaient pour les phrases, comme Skip-thought. Depuis 2018, le vecteur d'un m\u00eame mot change en fonction du reste de la phrase, on parle d'embeddings contextuels, ce qui permet de d\u00e9sambiguiser le sens du mot \"avocat\" par exemple. Les mod\u00e8les qui calculent ces vecteurs sont par exemple BERT, T5, BART, GPT, G-Shard, Wu Dao 2.0... Ces mod\u00e8les sont entra\u00een\u00e9s sur d'immenses bases de donn\u00e9es extraites du Web et sont les plus gros mod\u00e8les de deep learning aujourd'hui. Les vecteurs qu'ils produisent contiennent des informations sur tous les aspects linguistiques des mots (morphologie, syntaxe, s\u00e9mantique, etc.), mais aussi sur des faits li\u00e9s au mot (par exemple, tout ce que JFK a r\u00e9alis\u00e9 d'important dans sa vie). Les plus gros mod\u00e8les sont en anglais; il en existe \u00e9galement dans d'autres langues, mais les solutions les plus performantes restent aujourd'hui de soit traduire d'abord en anglais pour pouvoir utiliser les meilleurs mod\u00e8les, soit utiliser des mod\u00e8les multi-lingues comme XLMR, qui fusionnent et partagent l'information entre de nombreuses langues. Ces mod\u00e8les gigantesques offrent les meilleures performances, mais soul\u00e8vent de nombreux probl\u00e8mes \u00e9thiques (ils peuvent reproduire voire amplifier les biais des donn\u00e9es, ils contiennent des informations priv\u00e9es, ils sont utilis\u00e9s dans toutes les technologies en TAL mais seule une poign\u00e9e d'acteurs mondiaux les contr\u00f4le...) et \u00e9cologiques (le co\u00fbt-carbone pour les entra\u00eener est exhorbitant) qui focalisent l'attention des chercheurs depuis 2020. Voir aussi LLM .","title":"E"},{"location":"e/#embeddings","text":"( Fr: plongements ) Ce terme fait r\u00e9f\u00e9rence \u00e0 une repr\u00e9sentation vectorielle des observations. Il est souvent (mais pas seulement) utilis\u00e9 en TAL pour repr\u00e9senter les mots, ou les phrases, ou les documents, avec un seul vecteur. Avant 2018, un mot du lexique \u00e9tait souvent repr\u00e9sent\u00e9 par un unique vecteur, toujours le m\u00eame, par exemple calcul\u00e9 avec les mod\u00e8les Word2Vec ou Glove. D'autres mod\u00e8les existaient pour les phrases, comme Skip-thought. Depuis 2018, le vecteur d'un m\u00eame mot change en fonction du reste de la phrase, on parle d'embeddings contextuels, ce qui permet de d\u00e9sambiguiser le sens du mot \"avocat\" par exemple. Les mod\u00e8les qui calculent ces vecteurs sont par exemple BERT, T5, BART, GPT, G-Shard, Wu Dao 2.0... Ces mod\u00e8les sont entra\u00een\u00e9s sur d'immenses bases de donn\u00e9es extraites du Web et sont les plus gros mod\u00e8les de deep learning aujourd'hui. Les vecteurs qu'ils produisent contiennent des informations sur tous les aspects linguistiques des mots (morphologie, syntaxe, s\u00e9mantique, etc.), mais aussi sur des faits li\u00e9s au mot (par exemple, tout ce que JFK a r\u00e9alis\u00e9 d'important dans sa vie). Les plus gros mod\u00e8les sont en anglais; il en existe \u00e9galement dans d'autres langues, mais les solutions les plus performantes restent aujourd'hui de soit traduire d'abord en anglais pour pouvoir utiliser les meilleurs mod\u00e8les, soit utiliser des mod\u00e8les multi-lingues comme XLMR, qui fusionnent et partagent l'information entre de nombreuses langues. Ces mod\u00e8les gigantesques offrent les meilleures performances, mais soul\u00e8vent de nombreux probl\u00e8mes \u00e9thiques (ils peuvent reproduire voire amplifier les biais des donn\u00e9es, ils contiennent des informations priv\u00e9es, ils sont utilis\u00e9s dans toutes les technologies en TAL mais seule une poign\u00e9e d'acteurs mondiaux les contr\u00f4le...) et \u00e9cologiques (le co\u00fbt-carbone pour les entra\u00eener est exhorbitant) qui focalisent l'attention des chercheurs depuis 2020. Voir aussi LLM .","title":"Embeddings"},{"location":"f/","text":"Federated learning \uf0c1 Comment continuer \u00e0 entra\u00eener des mod\u00e8les sans divulguer les donn\u00e9es priv\u00e9es ? Comment se passer des fermes de GPU au co\u00fbt \u00e9cologique important ? Une solution \u00e0 ces deux probl\u00e8mes (et \u00e0 d'autres) pourrait bien \u00eatre le federated learning (FL), qui apprend les mod\u00e8les localement et les fusionne ensuite soit globalement, soit de mani\u00e8re d\u00e9centralis\u00e9e. D'innombrables variantes de ce principe existent, les plus connues \u00e9tant connues sous le nom de \"collaborative learning\" et \"fusion learning\". Il existe de nombreuses formes de FL: cross-silo lorsque les noeuds de calcul locaux sont puissants, cross-device lorsque l'essentiel du calcul est d\u00e9port\u00e9, horizontal lorsque les donn\u00e9es sont distribu\u00e9es ind\u00e9pendamment entre les noeuds, vertical lorsque les noeuds collaborent sur des donn\u00e9es en partie partag\u00e9es, etc. voir par exemple le papier \"A Secure and Efficient Federated Learning Framework for NLP\" et une premi\u00e8re \u00e9tude pour r\u00e9duire le co\u00fbt environnemental du deep learning . D'autres plateformes de Federated Learning sont list\u00e9es ici .","title":"F"},{"location":"f/#federated-learning","text":"Comment continuer \u00e0 entra\u00eener des mod\u00e8les sans divulguer les donn\u00e9es priv\u00e9es ? Comment se passer des fermes de GPU au co\u00fbt \u00e9cologique important ? Une solution \u00e0 ces deux probl\u00e8mes (et \u00e0 d'autres) pourrait bien \u00eatre le federated learning (FL), qui apprend les mod\u00e8les localement et les fusionne ensuite soit globalement, soit de mani\u00e8re d\u00e9centralis\u00e9e. D'innombrables variantes de ce principe existent, les plus connues \u00e9tant connues sous le nom de \"collaborative learning\" et \"fusion learning\". Il existe de nombreuses formes de FL: cross-silo lorsque les noeuds de calcul locaux sont puissants, cross-device lorsque l'essentiel du calcul est d\u00e9port\u00e9, horizontal lorsque les donn\u00e9es sont distribu\u00e9es ind\u00e9pendamment entre les noeuds, vertical lorsque les noeuds collaborent sur des donn\u00e9es en partie partag\u00e9es, etc. voir par exemple le papier \"A Secure and Efficient Federated Learning Framework for NLP\" et une premi\u00e8re \u00e9tude pour r\u00e9duire le co\u00fbt environnemental du deep learning . D'autres plateformes de Federated Learning sont list\u00e9es ici .","title":"Federated learning"},{"location":"g/","text":"GAN \uf0c1 Generative Adversarial Network: plut\u00f4t que g\u00e9n\u00e9rer des donn\u00e9es en minimisant simplement la distance entre les donn\u00e9es g\u00e9n\u00e9r\u00e9es et les exemples r\u00e9els (c'est l'apprentissage classique), un bien meilleur g\u00e9n\u00e9rateur peut \u00eatre obtenu en lui apprenant \u00e0 g\u00e9n\u00e9rer des exemples qui trompent un autre r\u00e9seau, qui est lui entra\u00een\u00e9 \u00e0 discriminer les exemples r\u00e9els des faux. Pas facile \u00e0 faire converger, mais d'innombrables progr\u00e8s ont \u00e9t\u00e9 r\u00e9alis\u00e9s (cf. par ex. Wasserstein-GAN). GPT \uf0c1 Famille de mod\u00e8les de langage, connus pour leur grande taille. Ils sont bas\u00e9s sur les transformer, mais n'utilisent que la partie d\u00e9codeur. Ce sont de purs mod\u00e8les de langage. GPT2 est disponible gratuitement, est de taille \"raisonnable\" et peut donc \u00eatre utilis\u00e9 sur un ordinateur personnel. GPT3 est beaucoup plus gros et n'est accessible que via une API payante. Des versions interm\u00e9diaires, comme Distill-GPT tentent de concilier performances et taille g\u00e9rable. Le r\u00e9cent GPT-J est tous gratuitement et donnerait des performances comp\u00e9titives pour une taille encore g\u00e9rable sur des syst\u00e8mes personnels assez puissants. Notons que l'utilisation de tels mod\u00e8les requiert de nombreuses optimisations, par exemple celles de la famille d'algorithmes Zero-2 et DeepSpeed, voire parfois de la parall\u00e9lisation sur plusieurs GPUs, toutes fonctions accessibles \u00e0 moindre effort via des librairies comme accelerate ou pytorch lightning .","title":"G"},{"location":"g/#gan","text":"Generative Adversarial Network: plut\u00f4t que g\u00e9n\u00e9rer des donn\u00e9es en minimisant simplement la distance entre les donn\u00e9es g\u00e9n\u00e9r\u00e9es et les exemples r\u00e9els (c'est l'apprentissage classique), un bien meilleur g\u00e9n\u00e9rateur peut \u00eatre obtenu en lui apprenant \u00e0 g\u00e9n\u00e9rer des exemples qui trompent un autre r\u00e9seau, qui est lui entra\u00een\u00e9 \u00e0 discriminer les exemples r\u00e9els des faux. Pas facile \u00e0 faire converger, mais d'innombrables progr\u00e8s ont \u00e9t\u00e9 r\u00e9alis\u00e9s (cf. par ex. Wasserstein-GAN).","title":"GAN"},{"location":"g/#gpt","text":"Famille de mod\u00e8les de langage, connus pour leur grande taille. Ils sont bas\u00e9s sur les transformer, mais n'utilisent que la partie d\u00e9codeur. Ce sont de purs mod\u00e8les de langage. GPT2 est disponible gratuitement, est de taille \"raisonnable\" et peut donc \u00eatre utilis\u00e9 sur un ordinateur personnel. GPT3 est beaucoup plus gros et n'est accessible que via une API payante. Des versions interm\u00e9diaires, comme Distill-GPT tentent de concilier performances et taille g\u00e9rable. Le r\u00e9cent GPT-J est tous gratuitement et donnerait des performances comp\u00e9titives pour une taille encore g\u00e9rable sur des syst\u00e8mes personnels assez puissants. Notons que l'utilisation de tels mod\u00e8les requiert de nombreuses optimisations, par exemple celles de la famille d'algorithmes Zero-2 et DeepSpeed, voire parfois de la parall\u00e9lisation sur plusieurs GPUs, toutes fonctions accessibles \u00e0 moindre effort via des librairies comme accelerate ou pytorch lightning .","title":"GPT"},{"location":"l/","text":"Language models \uf0c1 Les mod\u00e8les de langage pr\u00e9disent le prochain mot (ou caract\u00e8re, ou unit\u00e9 lexicale) \u00e0 partir d'une suite de mots. Avant 2012, ils prenaient souvent la forme de n-grammes. Avant 2018, c'\u00e9tait souvent des mod\u00e8les r\u00e9currents RNN ou LSTM. Apr\u00e8s 2018, ce sont souvent des Transformers. Ils ont toujours \u00e9t\u00e9 tr\u00e8s importants en TAL, mais ils le sont encore plus aujourd'hui, car les Large Language Models (LLM) qui dominent la plupart des t\u00e2ches de TAL aujourd'hui sont entra\u00een\u00e9s principalement avec cet objectif, ou un objectif similaire, comme les Masked Language Models (MLM). Les LLM les plus connus sont de la famille des mod\u00e8les GPT. Voir par exemple le papier \"Finetuned Language Models Are Zero-Shot Learners\".","title":"L"},{"location":"l/#language-models","text":"Les mod\u00e8les de langage pr\u00e9disent le prochain mot (ou caract\u00e8re, ou unit\u00e9 lexicale) \u00e0 partir d'une suite de mots. Avant 2012, ils prenaient souvent la forme de n-grammes. Avant 2018, c'\u00e9tait souvent des mod\u00e8les r\u00e9currents RNN ou LSTM. Apr\u00e8s 2018, ce sont souvent des Transformers. Ils ont toujours \u00e9t\u00e9 tr\u00e8s importants en TAL, mais ils le sont encore plus aujourd'hui, car les Large Language Models (LLM) qui dominent la plupart des t\u00e2ches de TAL aujourd'hui sont entra\u00een\u00e9s principalement avec cet objectif, ou un objectif similaire, comme les Masked Language Models (MLM). Les LLM les plus connus sont de la famille des mod\u00e8les GPT. Voir par exemple le papier \"Finetuned Language Models Are Zero-Shot Learners\".","title":"Language models"},{"location":"p/","text":"Programmation diff\u00e9rentiable \uf0c1 Le deep learning peut \u00eatre vu comme une extension de la programmation informatique. Un mod\u00e8le de deep learning est un programme contenant des param\u00e8tres, qui sont optimis\u00e9s automatiquement sur des donn\u00e9es. Cette optimisation est r\u00e9alis\u00e9e gr\u00e2ce \u00e0 des techniques de diff\u00e9rentiation automatique de la fonction math\u00e9matique cod\u00e9e par le programme. D'o\u00f9 le nom de \"programmation diff\u00e9rentiable\" qui a parfois \u00e9t\u00e9 donn\u00e9 au deep learning.","title":"P"},{"location":"p/#programmation-differentiable","text":"Le deep learning peut \u00eatre vu comme une extension de la programmation informatique. Un mod\u00e8le de deep learning est un programme contenant des param\u00e8tres, qui sont optimis\u00e9s automatiquement sur des donn\u00e9es. Cette optimisation est r\u00e9alis\u00e9e gr\u00e2ce \u00e0 des techniques de diff\u00e9rentiation automatique de la fonction math\u00e9matique cod\u00e9e par le programme. D'o\u00f9 le nom de \"programmation diff\u00e9rentiable\" qui a parfois \u00e9t\u00e9 donn\u00e9 au deep learning.","title":"Programmation diff\u00e9rentiable"},{"location":"t/","text":"T5 \uf0c1 T5 est le pr\u00e9curseur de T0pp , qui sont tous deux des mod\u00e8les seq2seq exploitant des transformeurs complets (encodeurs et d\u00e9codeurs), tout comme BART, et \u00e0 la diff\u00e9rente des mod\u00e8les de type BERT ou GPT. T0pp a \u00e9t\u00e9 optimis\u00e9 pour permettre le Zero-Shot Learning et le Prompt Programming. Voir aussi GPT . Time series \uf0c1 Les s\u00e9ries temporelles peuvent encore de nombreux d\u00e9fis aux mod\u00e8les de deep learning (d\u00e9pendances pouvant \u00eatre tr\u00e8s longues, signaux faibles...) et de nombreux mod\u00e8les sont imagin\u00e9s pour tenter de r\u00e9soudre ces probl\u00e8mes, comme les mod\u00e8les informer, autoformer, fedformer, etsformer, ssdnet, N-Hits, TS2Vec, ... Voir aussi le papier \"Transformer with Sparse Attention Mechanism for Industrial Time Series Forecasting\" Traitement automatique du langage naturel \uf0c1 Le TAL (Traitement Automatique du Langage Naturel) est un des domaines de recherche \u00e0 la pointe du deep learning depuis 2018. Il inclut des applications comme la traduction automatique, les chatbots, le r\u00e9sum\u00e9 automatique, la transcription de la parole, etc. Les plus gros mod\u00e8les de deep learning sont, et de tr\u00e8s loin, ceux en TAL. Voir aussi Embeddings Transformer \uf0c1 Le mod\u00e8le qui a compl\u00e8tement remplac\u00e9 toute la famille des r\u00e9seaux r\u00e9currents (RNN, LSTM, GRU...) par une nouvelle famille bas\u00e9e sur la self-attention. Pourquoi les transformers sont meilleurs ? Parce que les empiler permet de capturer l'information provenant de corpus textuels de plus en plus gigantesques, tout comme les CNN avec les gros corpus d'images. Les transformers ont r\u00e9volutionn\u00e9 le TAL, et sont \u00e0 la base de tous les mod\u00e8les r\u00e9cents du domaine: T0pp, GPT-J, BERT, GPT-1.2.3, RoBERTa, ALBERT, XL-NET, T5, BART... Transformers \uf0c1 Est le nom d'une des librairies python les plus utilis\u00e9es en 2022 pour acc\u00e9der et manipuler de tr\u00e8s nombreux mod\u00e8les d'embeddings. pip install transformers","title":"T"},{"location":"t/#t5","text":"T5 est le pr\u00e9curseur de T0pp , qui sont tous deux des mod\u00e8les seq2seq exploitant des transformeurs complets (encodeurs et d\u00e9codeurs), tout comme BART, et \u00e0 la diff\u00e9rente des mod\u00e8les de type BERT ou GPT. T0pp a \u00e9t\u00e9 optimis\u00e9 pour permettre le Zero-Shot Learning et le Prompt Programming. Voir aussi GPT .","title":"T5"},{"location":"t/#time-series","text":"Les s\u00e9ries temporelles peuvent encore de nombreux d\u00e9fis aux mod\u00e8les de deep learning (d\u00e9pendances pouvant \u00eatre tr\u00e8s longues, signaux faibles...) et de nombreux mod\u00e8les sont imagin\u00e9s pour tenter de r\u00e9soudre ces probl\u00e8mes, comme les mod\u00e8les informer, autoformer, fedformer, etsformer, ssdnet, N-Hits, TS2Vec, ... Voir aussi le papier \"Transformer with Sparse Attention Mechanism for Industrial Time Series Forecasting\"","title":"Time series"},{"location":"t/#traitement-automatique-du-langage-naturel","text":"Le TAL (Traitement Automatique du Langage Naturel) est un des domaines de recherche \u00e0 la pointe du deep learning depuis 2018. Il inclut des applications comme la traduction automatique, les chatbots, le r\u00e9sum\u00e9 automatique, la transcription de la parole, etc. Les plus gros mod\u00e8les de deep learning sont, et de tr\u00e8s loin, ceux en TAL. Voir aussi Embeddings","title":"Traitement automatique du langage naturel"},{"location":"t/#transformer","text":"Le mod\u00e8le qui a compl\u00e8tement remplac\u00e9 toute la famille des r\u00e9seaux r\u00e9currents (RNN, LSTM, GRU...) par une nouvelle famille bas\u00e9e sur la self-attention. Pourquoi les transformers sont meilleurs ? Parce que les empiler permet de capturer l'information provenant de corpus textuels de plus en plus gigantesques, tout comme les CNN avec les gros corpus d'images. Les transformers ont r\u00e9volutionn\u00e9 le TAL, et sont \u00e0 la base de tous les mod\u00e8les r\u00e9cents du domaine: T0pp, GPT-J, BERT, GPT-1.2.3, RoBERTa, ALBERT, XL-NET, T5, BART...","title":"Transformer"},{"location":"t/#transformers","text":"Est le nom d'une des librairies python les plus utilis\u00e9es en 2022 pour acc\u00e9der et manipuler de tr\u00e8s nombreux mod\u00e8les d'embeddings. pip install transformers","title":"Transformers"},{"location":"z/","text":"Zero-shot learning \uf0c1 En TAL, l'une des marottes du d\u00e9but 2021 consiste \u00e0 exploiter les gros transformers appris sur tout l'anglais (T5, BART...) pour r\u00e9soudre des t\u00e2ches de NLP sans apprentissage, simplement en ajoutant la description D de la t\u00e2che en anglais aux observations X , le mod\u00e8le \u00e9tant capable d'inf\u00e9rer le lien s\u00e9mantique entre D,X et la solution Y . Refs: adapters entailment","title":"Z"},{"location":"z/#zero-shot-learning","text":"En TAL, l'une des marottes du d\u00e9but 2021 consiste \u00e0 exploiter les gros transformers appris sur tout l'anglais (T5, BART...) pour r\u00e9soudre des t\u00e2ches de NLP sans apprentissage, simplement en ajoutant la description D de la t\u00e2che en anglais aux observations X , le mod\u00e8le \u00e9tant capable d'inf\u00e9rer le lien s\u00e9mantique entre D,X et la solution Y . Refs: adapters entailment","title":"Zero-shot learning"}]}