## BART

Famille de modèles seq2seq basés sur les transformers.
Voir [T5](../t/#T5).

## BERT

Un des modèles d'embeddings contextuels les plus connus, mais il commence à être
"ancien" dans la communauté TAL, et ses résultats ne sont plus au top.
Il est basé sur les transformers, mais n'utilise que la partie "encodeur".
Son apprentissage utilisait du Masked Language Modeling ([MLM](../m/#mlm)).
Des version pour le français ont été créées: CamemBERT et FlauBERT.
Le modèle RoBERTa est une évolution directe de BERT, plus performante.

Voir aussi [LLM](../l/#language-models) et [GPT](../g/#gpt).

