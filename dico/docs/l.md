## Language models

Les modèles de langage prédisent le prochain mot (ou caractère, ou unité lexicale)
à partir d'une suite de mots. Avant 2012, ils prenaient souvent la forme
de n-grammes. Avant 2018, c'était souvent des modèles récurrents RNN ou LSTM.
Après 2018, ce sont souvent des Transformers.
Ils ont toujours été très importants en TAL, mais ils le sont encore plus aujourd'hui,
car les Large Language Models (LLM) qui dominent la plupart des tâches de TAL aujourd'hui
sont entraînés principalement avec cet objectif, ou un objectif similaire, comme les
Masked Language Models (MLM).

Les LLM les plus connus sont de la famille des modèles GPT.

Voir par exemple le papier "Finetuned Language Models Are Zero-Shot Learners".

